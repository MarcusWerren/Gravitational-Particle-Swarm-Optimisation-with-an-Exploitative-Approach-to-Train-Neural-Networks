##########################################################################################################
# Title: Particle Swarm Optimsation Script
# Author: Marcus Werren
# Email: u17258627@tuks.co.za
# Script and data info: This script contains the back propagation algorithms for training and testing 
#an NN model.
# Note: Calculating the swarm diversity and average velocity for the swarm significantly increases the 
#execution time of the algorithms
##########################################################################################################

# PyTorch imports
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# Misc imports 
import numpy as np
import copy

# Self-created imports 
import NeuralNetwork 
import LoadData

# Used to create a particle object
class Particle():

	def __init__(self, neuralNetStruct, options):
		self.position = NeuralNetwork.NeuralNet(neuralNetStruct[0], neuralNetStruct[1], neuralNetStruct[2])
		self.prevPosition = self.position.copyModel() # initialise the previous location of the particle
		self.options = options

		# save the coefficients
		self.c1 = torch.tensor([options['c1']])
		self.c2 = torch.tensor([options['c2']])
		
		# Save the intertia weight
		self.w = torch.tensor([options['w']])
		
		# create the random vectors 
		self.r1 = NeuralNetwork.NeuralNet(neuralNetStruct[0], neuralNetStruct[1], neuralNetStruct[2]).createRandomModel()
		self.r2 = NeuralNetwork.NeuralNet(neuralNetStruct[0], neuralNetStruct[1], neuralNetStruct[2]).createRandomModel()

		# If the gravitaion attarctor is used 
		if len(self.options) == 4:
			self.c3 = torch.tensor([options['c3']])
			self.r3 = NeuralNetwork.NeuralNet(neuralNetStruct[0], neuralNetStruct[1], neuralNetStruct[2]).createRandomModel()
			self.centre = NeuralNetwork.NeuralNet(neuralNetStruct[0], neuralNetStruct[1], neuralNetStruct[2]).zeros() # create the center of the search space

		self.velocity = NeuralNetwork.NeuralNet(neuralNetStruct[0], neuralNetStruct[1], neuralNetStruct[2]).zeros()
		self.pBest = self.position # initialise the bet position to its selfs
		# self.gBest = self.position
		self.cost = torch.tensor([float('inf')]) # set the inital cost value

	# Update the velocity for a particle
	def updateVelocity(self, gBest):
		# Calculate the velocity for i+1
		with torch.no_grad():
			for i in range(2):
				# Update the velocity
				# w * v^t
				self.velocity.layer(i).weight.mul_(self.w.item())
				self.velocity.layer(i).bias.mul_(self.w.item())

				# c1 * r1 * (pBest - x^t)
				self.velocity.layer(i).weight.add_(self.c1.item() * self.r1.layer(i).weight * (self.pBest.layer(i).weight - self.position.layer(i).weight)) 
				self.velocity.layer(i).bias.add_(self.c1.item() * self.r1.layer(i).bias * (self.pBest.layer(i).bias - self.position.layer(i).bias)) 

				# c2 * r2 * (gBest - x^t)
				self.velocity.layer(i).weight.add_(self.c2.item() * self.r2.layer(i).weight * (gBest.layer(i).weight - self.position.layer(i).weight)) 
				self.velocity.layer(i).bias.add_(self.c2.item() * self.r2.layer(i).bias * (gBest.layer(i).bias - self.position.layer(i).bias)) 

				# Added gravitational attractor component 
				if len(self.options) == 4:
					# # c3 * r3 * (centre - x^t)
					self.velocity.layer(i).weight.add_(self.c3.item() * self.r3.layer(i).weight * (self.centre.layer(i).weight - self.position.layer(i).weight)) 
					self.velocity.layer(i).bias.add_(self.c3.item() * self.r3.layer(i).bias * (self.centre.layer(i).bias - self.position.layer(i).bias))

	# Update particles position 
	def updatePosition(self):
		self.prevPosition = self.position.copyModel()
		# Calculate the position for i+1 
		with torch.no_grad():
			for i in range(2):
				self.position.layer(i).weight.add_(self.velocity.layer(i).weight)
				self.position.layer(i).bias.add_(self.velocity.layer(i).bias)

# Used to create the swarm object
class Swarm():

	def __init__(self, neuralNetStruct, options, population_size):
		self.options = options
		self.neuralNetStruct = neuralNetStruct
		self.population_size = population_size

		# Create the population
		self.swarm = []
		for i in range(population_size):
			self.swarm.append(Particle(neuralNetStruct, options))

		# set gBest to inital values
		self.gBest = None
		self.gBest_cost = torch.tensor([float('inf')]) 

	# Function to find the best particle
	def findGlobalBest(self, samples, labels, criterion):
		
		cost_best = torch.tensor([float('inf')]) 

		for particle in self.swarm:		
			cost = self.calculate_cost(particle.position, samples, labels, criterion)
			if cost.item() < cost_best.item():
				cost_best = cost
				self.gBest = particle.position.copyModel()

		return self.gBest

	# Helper function to get the a particle 
	def __getitem__(self, index):
		return self.swarm[index]

	# helper function to get the size of the swarm
	def __len__(self):
		return len(self.swarm)

	# Function to calclate the cost 
	def calculate_cost(self, position, samples, labels, criterion):
		outputs = position(samples)
		cost = criterion(outputs, labels)
		return cost

	# Helper function to calculate the euclidean distance between two particles
	def euclidDist(self, vec1, vec2):
		first = NeuralNetwork.NeuralNet(self.neuralNetStruct[0], self.neuralNetStruct[1], self.neuralNetStruct[2]).zeros()
		euclid = torch.tensor([0.0])

		with torch.no_grad():
			for i in range(2):
				first.layer(i).weight.add_(vec2.layer(i).weight)
				first.layer(i).weight.sub_(vec1.layer(i).weight)
				first.layer(i).weight.mul_(first.layer(i).weight)

				first.layer(i).bias.add_(vec2.layer(i).bias)
				first.layer(i).bias.sub_(vec1.layer(i).bias)
				first.layer(i).bias.mul_(first.layer(i).bias)

				euclid.add_(torch.sum(first.layer(i).weight))
				euclid.add_(torch.sum(first.layer(i).bias))

		return torch.sqrt(euclid).item()

	# Function for dynamically changing C3 value
	def updateC3(self, iters):
		c3 = torch.tensor([ ( math.exp((-1 * (iters+1))/10) ) * 0.75])

		for particle in self.swarm:
			particle.c3.data = c3

	# Function to calculate the average particle velocity
	def averageParticleVelocity(self):
		total = []

		for particle in self.swarm:
			total.append(self.euclidDist(particle.position, particle.prevPosition))

		return sum(total) / len(total)

	# Function to calculate the swarm diversity
	def swarmDiversity(self):
		total = []

		for i in range(self.population_size):
			for j in range(i, self.population_size, 1):
				if i != j:
					total.append(self.euclidDist(self.swarm[i].position, self.swarm[j].position))

		return sum(total) / len(total)

class ParticleSwarmOptimisation():

	def __init__(self, neuralNetStruct, options, population_size, num_iter, dataSetName, path, batch_size=0):
		self.neuralNetStruct = neuralNetStruct
		self.batch_size = batch_size
		self.options = options
		self.population_size = population_size
		self.num_iter = num_iter
		self.dataSetName = dataSetName
		self.path =  path

		# Percentage split between training, test and validation set sizes (i.e. Train - 70%, Test - 15% and Validation - 15%)
		self.dataSplitTT = [0.70001, 0.29999]
		self.dataSplitTV = [0.50001, 0.49999]

		# Extract the data
		if dataSetName == 'iris':
			self.dataset = LoadData.IrisDataset(path)
		elif dataSetName == 'balance_scale':
			self.dataset = LoadData.BalanceScaleDataset(path)
		elif dataSetName == 'bank_notes':
			self.dataset = LoadData.BankNoteDataset(path)
		elif dataSetName == 'breast_cancer':
			self.dataset = LoadData.BreastCancerDataset(path)
		elif dataSetName == 'seeds':
			self.dataset = LoadData.SeedsDataset(path)
		elif dataSetName == 'hypersphere':
			self.dataset = LoadData.HypersphereDataset(path)

		# Split the data into test and train datasets
		train_data ,test_data = torch.utils.data.random_split(self.dataset, [round(len(self.dataset)*self.dataSplitTT[0]), round(len(self.dataset)*self.dataSplitTT[1])])
		test_data, valid_data = torch.utils.data.random_split(test_data, [round(len(test_data)*self.dataSplitTV[0]), round(len(test_data)*self.dataSplitTV[1])])

		# Prepare the data 
		for j in range(train_data[:][0].size()[1]):
			sumNum = torch.sum(torch.index_select(train_data[:][0], 1, torch.tensor([j]))).div_(train_data[:][0].size()[0])
			stdev = torch.std(torch.index_select(train_data[:][0], 1, torch.tensor([j])), unbiased=False)

			for i in range(train_data[:][0].size()[0]):
				(train_data[i][0][j].sub_(sumNum)).div_(stdev)

				if i < len(test_data):
					(test_data[i][0][j].sub_(sumNum)).div_(stdev) 
				
				if i < len(valid_data):
					(valid_data[i][0][j].sub_(sumNum)).div_(stdev) 


		# If the batch_size == 0 do not train in batches
		if self.batch_size == 0:
			self.batch_size = len(train_data)

		# Send the data into dataloaders
		self.train_loader = DataLoader(dataset=train_data, batch_size=self.batch_size, shuffle=True)
		self.test_loader = DataLoader(dataset=test_data, batch_size=self.batch_size, shuffle=False)
		self.valid_loader = DataLoader(dataset=valid_data, batch_size=self.batch_size, shuffle=False)

		# Create the population
		self.swarm = Swarm(neuralNetStruct, options, population_size)

		# Loss function
		self.criterion = nn.CrossEntropyLoss()

		# Initalise the gBest's for the swarm
		for j, (samples, labels) in enumerate(self.train_loader):
			self.swarm.findGlobalBest(samples, labels, self.criterion)

	# Function to create a new population
	def newPopulation(self):
		
		self.swarm = Swarm(self.neuralNetStruct, self.options, self.population_size)
		for j, (samples, labels) in enumerate(self.train_loader):
			self.swarm.findGlobalBest(samples, labels, self.criterion)

	# Function to shuffle the data
	def shuffleData(self):
		# Split the data into test and train datasets
		train_data ,test_data = torch.utils.data.random_split(self.dataset, [round(len(self.dataset)*self.dataSplitTT[0]), round(len(self.dataset)*self.dataSplitTT[1])])
		test_data, valid_data = torch.utils.data.random_split(test_data, [round(len(test_data)*self.dataSplitTV[0]), round(len(test_data)*self.dataSplitTV[1])])
		
		# Send the data into dataloaders
		self.train_loader = DataLoader(dataset=train_data, batch_size=self.batch_size, shuffle=True)
		self.test_loader = DataLoader(dataset=test_data, batch_size=self.batch_size, shuffle=False)
		self.valid_loader = DataLoader(dataset=valid_data, batch_size=self.batch_size, shuffle=False)


	# Validate the model
	def validateModel(self):
		with torch.no_grad():
			num_correct = 0 
			num_samples = 0

			for samples, labels in self.valid_loader:
				outputs = self.swarm.gBest(samples)

				# value and index
				_, predictions = torch.max(outputs, 1)
				
				num_samples += labels.shape[0]
				num_correct += (predictions == labels).sum().item()
			
			acc = 100.0 * num_correct / num_samples

			return acc # return the models validation accuracy

	# The training loop
	def trainPopulation(self, printSteps=False):

		costList = []
		validList = []
		bestCostList = []
		avgVelocity = []
		swmDivercity = []
		# iters = 0

		for i in range(self.num_iter):
			totalCost = 0
			
			# For each batch 
			for j, (samples, labels) in enumerate(self.train_loader):
				
				# For each particle in the swarm
				for particle in self.swarm:
					
					currCost = self.swarm.calculate_cost(particle.position, samples, labels, self.criterion) # Calcalate the current cost
					
					totalCost +=  currCost.item()
					
					if printSteps == True:
						print(f'curr:{currCost.item()}    \tpBest:{particle.cost.item()}    \tgBest:{self.swarm.gBest_cost.item()}')

					# test for the pBest
					if currCost < particle.cost:
						particle.pBest = particle.position.copyModel()
						particle.cost = currCost

					# test for the gBest
					if currCost < self.swarm.gBest_cost:
						self.swarm.gBest = particle.position.copyModel()
						self.swarm.gBest_cost = currCost
					
					# Update the velocity and the position
					particle.updateVelocity(self.swarm.gBest)			
					particle.updatePosition()


			# self.swarm.updateC3(iters)
			# iters += 1
					
			# validList.append(self.validateModel())
			costList.append(totalCost / self.population_size)
			bestCostList.append(self.swarm.gBest_cost.item())
			avgVelocity.append(self.swarm.averageParticleVelocity())
			swmDivercity.append(self.swarm.swarmDiversity())			
			
		return costList, validList, bestCostList, avgVelocity, swmDivercity


	# Test the model
	def testPopulation(self, printAcc=False):
		# test
		with torch.no_grad():
			num_correct = 0 
			num_samples = 0

			for samples, labels in self.test_loader:
				outputs = self.swarm.gBest(samples)

				# value and index
				_, predictions = torch.max(outputs, 1)
				
				num_samples += labels.shape[0]
				num_correct += (predictions == labels).sum().item()
			
			acc = 100.0 * num_correct / num_samples
			
			if printAcc == True:
				print(f"test accuray = {acc}")

			return acc # return the models test accuracy