##########################################################################################################
# Title: Back Propagation Script
# Author: Marcus Werren
# Email: u17258627@tuks.co.za
# Script and data info: This script contains the back propagation algorithm for training and testing an 
#NN model.
# Note:
##########################################################################################################

# PyTorch imports
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# Misc imports
import matplotlib.pyplot as plt

# Self-created imports 
import NeuralNetwork 
import LoadData

class BackPropagation():

	def __init__(self, neuralNetStruct, batch_size, learning_rate, num_epochs, dataSetName, path):
		self.neuralNetStruct = neuralNetStruct
		self.batch_size = batch_size
		self.learning_rate = learning_rate
		self.num_epochs = num_epochs
		self.dataSetName = dataSetName
		self.path =  path

		# Percentage split between training, test and validation set sizes (i.e. Train - 70%, Test - 15% and Validation - 15%)
		self.dataSplitTT = [0.70001, 0.29999]
		self.dataSplitTV = [0.50001, 0.49999]


		# Extract the data
		if dataSetName == 'iris':
			self.dataset = LoadData.IrisDataset(path)
		elif dataSetName == 'balance_scale':
			self.dataset = LoadData.BalanceScaleDataset(path)
		elif dataSetName == 'bank_notes':
			self.dataset = LoadData.BankNoteDataset(path)
		elif dataSetName == 'breast_cancer':
			self.dataset = LoadData.BreastCancerDataset(path)
		elif dataSetName == 'seeds':
			self.dataset = LoadData.SeedsDataset(path)
		elif dataSetName == 'hypersphere':
			self.dataset = LoadData.HypersphereDataset(path)

		# Split the data into test and train datasets
		train_data ,test_data = torch.utils.data.random_split(self.dataset, [round(len(self.dataset)*self.dataSplitTT[0]), round(len(self.dataset)*self.dataSplitTT[1])])
		test_data, valid_data = torch.utils.data.random_split(test_data, [round(len(test_data)*self.dataSplitTV[0]), round(len(test_data)*self.dataSplitTV[1])])

		# Prepare the data 
		for j in range(train_data[:][0].size()[1]):
			sumNum = torch.sum(torch.index_select(train_data[:][0], 1, torch.tensor([j]))).div_(train_data[:][0].size()[0])
			stdev = torch.std(torch.index_select(train_data[:][0], 1, torch.tensor([j])), unbiased=False)

			for i in range(train_data[:][0].size()[0]):
				(train_data[i][0][j].sub_(sumNum)).div_(stdev)

				if i < len(test_data):
					(test_data[i][0][j].sub_(sumNum)).div_(stdev) 
				
				if i < len(valid_data):
					(valid_data[i][0][j].sub_(sumNum)).div_(stdev) 



		# If batch_size == 0 do not train in batches
		if self.batch_size != 0:
			self.batch_size = len(train_data)

		# Send the data into dataloaders
		self.train_loader = DataLoader(dataset=train_data, batch_size=self.batch_size, shuffle=True)
		self.test_loader = DataLoader(dataset=test_data, batch_size=self.batch_size, shuffle=False)
		self.valid_loader = DataLoader(dataset=valid_data, batch_size=self.batch_size, shuffle=False)


		# Create the model
		self.model = NeuralNetwork.NeuralNet(neuralNetStruct[0], neuralNetStruct[1], neuralNetStruct[2])

		# Loss and optimiiser functions
		self.criterion = nn.CrossEntropyLoss()
		self.optimiser = torch.optim.Adam(self.model.parameters(), lr=learning_rate)

	# helper function to initialise a new model
	def newModel(self):
		# Create the model
		self.model = NeuralNetwork.NeuralNet(self.neuralNetStruct[0], self.neuralNetStruct[1], self.neuralNetStruct[2])

		# Loss and optimiiser
		self.criterion = nn.CrossEntropyLoss()
		self.optimiser = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)

	# helper function to shuffle the data for a new run 
	def shuffleData(self):
		# Split the data into test and train datasets
		train_data ,test_data = torch.utils.data.random_split(self.dataset, [round(len(self.dataset)*self.dataSplitTT[0]), round(len(self.dataset)*self.dataSplitTT[1])])
		test_data, valid_data = torch.utils.data.random_split(test_data, [round(len(test_data)*self.dataSplitTV[0]), round(len(test_data)*self.dataSplitTV[1])])
		
		# Send the data into dataloaders
		self.train_loader = DataLoader(dataset=train_data, batch_size=self.batch_size, shuffle=True)
		self.test_loader = DataLoader(dataset=test_data, batch_size=self.batch_size, shuffle=False)
		self.valid_loader = DataLoader(dataset=valid_data, batch_size=self.batch_size, shuffle=False)

	# Validate the model
	def validateModel(self):
		with torch.no_grad():
			num_correct = 0 
			num_samples = 0

			for samples, labels in self.valid_loader:
				outputs = self.model(samples)

				# value and index
				_, predictions = torch.max(outputs, 1)
				
				num_samples += labels.shape[0]
				num_correct += (predictions == labels).sum().item()
			
			acc = 100.0 * num_correct / num_samples

			return acc # return the models validation accuracy 

	# Training loop
	def trainModel(self, printSteps=False):

		num_total_steps = len(self.train_loader)

		lossList = []
		validList = []

		for epoch in range(self.num_epochs):
			# self.train_loader = DataLoader(dataset=self.train_data, batch_size=self.batch_size, shuffle=True) # Shuffle the training data
			for i, (samples, labels) in enumerate(self.train_loader):
				
				# forward  
				outputs = self.model(samples)
				loss = self.criterion(outputs, labels)

				lossList.append(loss.item())

				# backward 
				self.optimiser.zero_grad()
				loss.backward()
				self.optimiser.step()

				# Print out the steps of training 
				if (i+1) % 1 == 0 and printSteps == True:
					print(f"epoch {epoch+1}/{self.num_epochs}, step {i+1}/{num_total_steps}, loss = {loss.item():.4f}")

			# validList.append(self.validateModel()) # Show validation accuracy after each epoch 

		return lossList, validList

	# Test the model
	def testModel(self, printAcc=False):

		with torch.no_grad():
			num_correct = 0 
			num_samples = 0

			for samples, labels in self.test_loader:
				outputs = self.model(samples)

				# value and index
				_, predictions = torch.max(outputs, 1)
				
				num_samples += labels.shape[0]
				num_correct += (predictions == labels).sum().item()

			
			acc = 100.0 * num_correct / num_samples
			
			if printAcc == True:
				print(f"test accuray = {acc}")

			return acc # return the models test accuracy 
